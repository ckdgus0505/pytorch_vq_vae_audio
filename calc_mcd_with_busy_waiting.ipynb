{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from wavenet_vocoder.wavenet import WaveNet\n",
    "from wavenet_vocoder.wavenet import receptive_field_size\n",
    "import librosa\n",
    "import pysptk\n",
    "from scipy.io import wavfile\n",
    "from fastdtw import fastdtw\n",
    "import pyworld\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./2_speaker/vctk_train.txt', 'r') as f:\n",
    "    data = f.read()\n",
    "file = data.splitlines()\n",
    "speaker_dic = {}\n",
    "number_of_speakers = 0\n",
    "for i in range (0, len(file)):\n",
    "    if (file[i].split('/')[0] in speaker_dic):\n",
    "        continue\n",
    "    else :\n",
    "        speaker_dic[file[i].split('/')[0]] = number_of_speakers\n",
    "        number_of_speakers+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizerEMA(nn.Module):\n",
    "    \"\"\"We will also implement a slightly modified version  which will use exponential moving averages\n",
    "    to update the embedding vectors instead of an auxillary loss.\n",
    "    This has the advantage that the embedding updates are independent of the choice of optimizer \n",
    "    for the encoder, decoder and other parts of the architecture.\n",
    "    For most experiments the EMA version trains faster than the non-EMA version.\"\"\"\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay, epsilon=1e-5):\n",
    "        super(VectorQuantizerEMA, self).__init__()\n",
    "        \n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._num_embeddings = num_embeddings\n",
    "        \n",
    "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
    "        #self._embedding.weight.data.normal_()\n",
    "        self._embedding.weight.data.uniform_(-1./512, 1./512)\n",
    "#        self._embedding.weight.data = torch.Tensor([0])\n",
    "        #self._embedding.weight.data = torch.Tensor(np.zeros(()))\n",
    "        self._commitment_cost = commitment_cost\n",
    "        \n",
    "        self.register_buffer('_ema_cluster_size', torch.zeros(num_embeddings))\n",
    "        self._ema_w = nn.Parameter(torch.Tensor(num_embeddings, self._embedding_dim))\n",
    "        self._ema_w.data.normal_()\n",
    "        \n",
    "        self._decay = decay\n",
    "        self._epsilon = epsilon\n",
    "    '''\n",
    "    def forward(self, inputs):\n",
    "        # convert inputs from BCL -> BLC\n",
    "        inputs = inputs.permute(0, 2, 1).contiguous()\n",
    "        input_shape = inputs.shape\n",
    "        # Flatten input\n",
    "        flat_input = inputs.view(-1, self._embedding_dim)     #[BL, C]\n",
    "        if (self._embedding.weight.data == 0).all():\n",
    "            self._embedding.weight.data = flat_input[-self._num_embeddings:].detach()\n",
    "        # Calculate distances\n",
    "\n",
    "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) \n",
    "                    + torch.sum(self._embedding.weight**2, dim=1)\n",
    "                    - 2 * torch.matmul(flat_input, self._embedding.weight.t())) #[BL, num_embeddings]\n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1) #[BL, 1]\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings).to(device)# [BL, num_embeddings]\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "        #print(encodings.shape) [250, 512]\n",
    "        # Use EMA to update the embedding vectors\n",
    "        if self.training:\n",
    "            self._ema_cluster_size = self._ema_cluster_size * self._decay + \\\n",
    "                                     (1 - self._decay) * torch.sum(encodings, 0)\n",
    "            #print(self._ema_cluster_size.shape) [512]\n",
    "            n = torch.sum(self._ema_cluster_size)\n",
    "            self._ema_cluster_size = (\n",
    "                (self._ema_cluster_size + self._epsilon)\n",
    "                / (n + self._num_embeddings * self._epsilon) * n)\n",
    "            \n",
    "            dw = torch.matmul(encodings.t(), flat_input)\n",
    "            self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw)\n",
    "            \n",
    "            self._embedding.weight = nn.Parameter(self._ema_w / self._ema_cluster_size.unsqueeze(1))\n",
    "        \n",
    "        # Quantize and unflatten\n",
    "        #encodings.shape = [BL, num_embeddings] , weight.shape=[num_embeddings, C]\n",
    "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
    "\n",
    "        \n",
    "        # Loss\n",
    "        e_latent_loss = torch.mean((quantized.detach() - inputs)**2)\n",
    "        q_latent_loss = torch.mean((quantized - inputs.detach())**2)\n",
    "#        print(q_latent_loss.item(), 0.25 * e_latent_loss.item())\n",
    "        loss = q_latent_loss + self._commitment_cost * e_latent_loss\n",
    "        \n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "        # convert quantized from BLC -> BCL\n",
    "        return loss, quantized.permute(0, 2, 1).contiguous(), perplexity\n",
    "    '''\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # convert inputs from BCL -> BLC\n",
    "        inputs = inputs.permute(0, 2, 1).contiguous()\n",
    "        input_shape = inputs.shape\n",
    "        # Flatten input\n",
    "        flat_input = inputs.view(-1, self._embedding_dim)     #[BL, C]\n",
    "        # Calculate distances\n",
    "        \n",
    "        distances = torch.norm(flat_input.unsqueeze(1) - self._embedding.weight, dim=2, p=2)\n",
    " #       distances = (torch.sum(flat_input**2, dim=1, keepdim=True) \n",
    " #                   + torch.sum(self._embedding.weight**2, dim=1)\n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1) #[BL, 1]\n",
    "        \n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings).to(device)# [BL, num_embeddings]\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "        #print(encodings.shape) [250, 512]\n",
    "\n",
    "#         # Use EMA to update the embedding vectors\n",
    "#         if self.training:\n",
    "#             self._ema_cluster_size = self._ema_cluster_size * self._decay + \\\n",
    "#                                      (1 - self._decay) * torch.sum(encodings, 0)\n",
    "#             #print(self._ema_cluster_size.shape) [512]\n",
    "#             n = torch.sum(self._ema_cluster_size)\n",
    "#             self._ema_cluster_size = (\n",
    "#                 (self._ema_cluster_size + self._epsilon)\n",
    "#                 / (n + self._num_embeddings * self._epsilon) * n)\n",
    "            \n",
    "#             dw = torch.matmul(encodings.t(), flat_input)\n",
    "#             self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw)\n",
    "            \n",
    "#             self._embedding.weight = nn.Parameter(self._ema_w / self._ema_cluster_size.unsqueeze(1))\n",
    "\n",
    "        # Quantize and unflatten\n",
    "        #encodings.shape = [BL, num_embeddings] , weight.shape=[num_embeddings, C]\n",
    "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
    "        # Loss\n",
    "        e_latent_loss = torch.mean((quantized.detach() - inputs)**2)\n",
    "        q_latent_loss = torch.mean((quantized - inputs.detach())**2)\n",
    "#        print(q_latent_loss.item(), 0.25 * e_latent_loss.item())\n",
    "        loss = q_latent_loss + self._commitment_cost * e_latent_loss\n",
    "        \n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "        # same as torch.exp( entropy loss )\n",
    "        \n",
    "        # convert quantized from BLC -> BCL\n",
    "        return loss, quantized.permute(0, 2, 1).contiguous(), perplexity\n",
    "#    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Audio encoder\n",
    "    The vq-vae paper says that the encoder has 6 strided convolutions with stride 2 and window-size 4.\n",
    "    The number of channels and a nonlinearity is not specified in the paper. \n",
    "    I tried using ReLU, it didn't work.\n",
    "    Now I try using tanh, hoping that this will keep my encoded values within the neighborhood of 0,\n",
    "    so they do not drift too far away from encoding vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoding_channels, in_channels=256):\n",
    "        super(Encoder,self).__init__()\n",
    "        self._num_layers = 2 * len(encoding_channels)\n",
    "        self._layers = nn.ModuleList()\n",
    "        for out_channels in encoding_channels:\n",
    "            self._layers.append(nn.Conv1d(in_channels=in_channels,\n",
    "                                    out_channels=out_channels,\n",
    "                                    stride=2,\n",
    "                                    kernel_size=4,\n",
    "                                    padding=0, \n",
    "                                        ))\n",
    "            self._layers.append(nn.Tanh())\n",
    "            in_channels = out_channels\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(self._num_layers):\n",
    "            x = self._layers[i](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoding_channels,\n",
    "                 num_embeddings, \n",
    "                 embedding_dim,\n",
    "                 commitment_cost, \n",
    "                 layers,\n",
    "                 stacks,\n",
    "                 kernel_size,\n",
    "                 decay=0):\n",
    "        super(Model, self).__init__()       \n",
    "        self._encoder = Encoder(encoding_channels=encoding_channels)\n",
    "        #I tried adding batch normalization here, because:\n",
    "        #the distribution of encoded values needs to be similar to the distribution of embedding vectors\n",
    "        #otherwise we'll see \"posterior collapse\": all values will be assigned to the same embedding vector,\n",
    "        #and stay that way (because vectors which do not get assigned anything do not get updated).\n",
    "        #Batch normalization is a way to fix that. But it didn't work: model\n",
    "        #reproduced voice correctly, but the words were completely wrong.\n",
    "        #self._batch_norm = nn.BatchNorm1d(1)\n",
    "\n",
    "        self._vq_vae = VectorQuantizerEMA(num_embeddings, embedding_dim, \n",
    "                                               commitment_cost, decay)\n",
    "\n",
    "        self._decoder = WaveNet(device, out_channels=256, #dimension of ohe mu-quantized signal\n",
    "                                layers=layers, #like in original WaveNet\n",
    "                                stacks=stacks,\n",
    "                                residual_channels=512,\n",
    "                                gate_channels=512,\n",
    "                                skip_out_channels=512,\n",
    "                                kernel_size=kernel_size, \n",
    "                                dropout=1 - 0.95,\n",
    "                                cin_channels=embedding_dim, #local conditioning channels - on encoder output\n",
    "                                gin_channels=number_of_speakers, #global conditioning channels - on speaker_id\n",
    "                                n_speakers=number_of_speakers,\n",
    "                                weight_normalization=False, \n",
    "                                upsample_conditional_features=True, \n",
    "                                decoding_channels=encoding_channels[::-1],\n",
    "                                use_speaker_embedding=False\n",
    "                               )\n",
    "        self.recon_loss = torch.nn.CrossEntropyLoss()\n",
    "        self.receptive_field = receptive_field_size(total_layers=layers, num_cycles=stacks, kernel_size=kernel_size)\n",
    "#        self.mean = None\n",
    "#        self.std = None\n",
    "    def forward(self, x):\n",
    "        audio, target, speaker_id = x\n",
    "        assert len(audio.shape) == 3 # B x C x L \n",
    "        assert audio.shape[1] == 256\n",
    "        z = self._encoder(audio)\n",
    "        #normalize output - subtract mean, divide by standard deviation\n",
    "        #without this, perplexity goes to 1 almost instantly\n",
    "#         if self.mean is None:\n",
    "#             self.mean = z.mean().detach()\n",
    "#         if self.std is None:\n",
    "#              self.std = z.std().detach()\n",
    "#        z = z - self.mean\n",
    "#        z = z / self.std\n",
    "        vq_loss, quantized, perplexity = self._vq_vae(z)\n",
    "#        assert z.shape == quantized.shape\n",
    "#        print(\"audio.shape\", audio.shape)\n",
    "#        print(\"quantized.shape\", quantized.shape)\n",
    "        x_recon = self._decoder(audio, quantized, speaker_id, softmax=False)\n",
    "        x_recon = x_recon[:, :, self.receptive_field:-1]\n",
    "        recon_loss_value = self.recon_loss(x_recon, target[:, 1:])\n",
    "        loss = recon_loss_value + vq_loss\n",
    "        \n",
    "        return loss, recon_loss_value, x_recon, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_updates = 39818\n",
    "#vector quantizer parameters:\n",
    "embedding_dim = 64 #dimension of each vector\n",
    "encoding_channels = [512,512,512,512,512,embedding_dim]\n",
    "num_embeddings = 512 #number of vectors\n",
    "commitment_cost = 0.25\n",
    "\n",
    "#wavenet parameters:\n",
    "kernel_size=2\n",
    "total_layers=30\n",
    "num_cycles=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "receptive_field = receptive_field_size(total_layers=total_layers, num_cycles=num_cycles, kernel_size=kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(num_embeddings=num_embeddings,\n",
    "              encoding_channels=encoding_channels,\n",
    "              embedding_dim=embedding_dim, \n",
    "              commitment_cost=commitment_cost, \n",
    "              layers=total_layers,\n",
    "              stacks=num_cycles,\n",
    "              kernel_size=kernel_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mu_law_encode(data, mu):\n",
    "    mu_x = np.sign(data) * np.log(1 + mu * np.abs(data)) / np.log(mu + 1)\n",
    "    return mu_x\n",
    "\n",
    "def mu_law_decode(mu_x, mu):\n",
    "    data = np.sign(mu_x) * (1 / mu) * ((1 + mu) ** np.abs(mu_x) - 1)\n",
    "    return data\n",
    "\n",
    "def quantize_data(data, classes):\n",
    "    mu_x = mu_law_encode(data, classes)\n",
    "    bins = np.linspace(-1, 1, classes)\n",
    "    quantized = np.digitize(mu_x, bins) - 1\n",
    "    return quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversion(original_wav, speaker):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated_file = np.array([])\n",
    "        wav, sr = librosa.load(original_wav)\n",
    "        speaker_index = speaker_dic[speaker]\n",
    "        \n",
    "        normalized = librosa.util.normalize(wav)\n",
    "        quantized = quantize_data(normalized, 256)\n",
    "\n",
    "        for i in range(0, len(wav), 16126):\n",
    "            sample = quantized[i:i+16126]\n",
    "            if (len(sample)!= 16126):\n",
    "                sample = np.append(sample, np.zeros(16126 - len(sample)).astype(int))\n",
    "                \n",
    "            sample = torch.from_numpy(sample)\n",
    "            ohe_audio = torch.FloatTensor(256, 16126).zero_()\n",
    "            ohe_audio.scatter_(0, sample.unsqueeze(0), 1.)\n",
    "            \n",
    "            speaker_id = torch.from_numpy(np.array(speaker_index)).unsqueeze(0).unsqueeze(0)\n",
    "            ohe_speaker = torch.FloatTensor(number_of_speakers, 1).zero_()\n",
    "            ohe_speaker.scatter_(0, speaker_id, 1.)\n",
    "\n",
    "            ohe_audio = ohe_audio.unsqueeze(0).to(device)\n",
    "            ohe_speaker = ohe_speaker.unsqueeze(0).to(device)\n",
    "            encoded = model._encoder(ohe_audio)\n",
    "            _, valid_quantize, _ = model._vq_vae(encoded)\n",
    "            \n",
    "            valid_reconstructions = model._decoder.incremental_forward(ohe_audio[:,:,0:1], \n",
    "                                                               valid_quantize, \n",
    "                                                               ohe_speaker, \n",
    "                                                               T=16126)\n",
    "            recon = valid_reconstructions.squeeze().argmax(dim=0).detach().cpu().numpy()\n",
    "            mu_encoded = (recon + 1) / 128 - 1\n",
    "            mu_decoded = mu_law_decode(recon, mu=256)\n",
    "            generated_file = np.append(generated_file, mu_encoded)\n",
    "        # librosa.output.write_wav(\"generated.wav\",  generated_file, sr=sr)\n",
    "        return wav, generated_file, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav_extract_mcep(converted_dir, sent):\n",
    "    wav, sr = librosa.load(os.path.join(converted_dir, sent), sr=22050, mono=True)\n",
    "    #_, wav = wavfile.read(os.path.join(converted_dir, sent))\n",
    "    \n",
    "    _, _, sp, _ = world_decompose(wav=wav, fs=22050, frame_period=5.0)\n",
    "    mcep = pysptk.sp2mc(sp, 36 - 1, 0.455)\n",
    "    return mcep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mcep(wav):\n",
    "    _, _, sp, _ = world_decompose(wav=wav, fs=22050, frame_period=5.0)\n",
    "    mcep = pysptk.sp2mc(sp, 36 - 1, 0.455)\n",
    "    return mcep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def world_decompose(wav, fs, frame_period = 5.0):\n",
    "\n",
    "    # Decompose speech signal into f0, spectral envelope and aperiodicity using WORLD\n",
    "    wav = wav.astype(np.float64)\n",
    "    f0, timeaxis = pyworld.harvest(wav, fs, frame_period = frame_period, f0_floor = 71.0, f0_ceil = 800.0)\n",
    "    sp = pyworld.cheaptrick(wav, f0, timeaxis, fs)\n",
    "    ap = pyworld.d4c(wav, f0, timeaxis, fs)\n",
    "\n",
    "    return f0, timeaxis, sp, ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcd_cal(converted_mcep, target_mcep):\n",
    "    converted_mcep = converted_mcep[:,1:]\n",
    "    target_mcep = target_mcep[:,1:]\n",
    "    twf = estimate_twf(converted_mcep, target_mcep, fast=True)\n",
    "    converted_mcep_mod = converted_mcep[twf[0]]\n",
    "    target_mcep_mod = target_mcep[twf[1]]\n",
    "    mcd = melcd(converted_mcep_mod, target_mcep_mod)\n",
    "    return mcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_twf(orgdata, tardata, distance='melcd', fast=True, otflag=None):\n",
    "    if distance == 'melcd':\n",
    "        def distance_func(x, y): return melcd(x, y)\n",
    "    else:\n",
    "        raise ValueError('other distance metrics than melcd does not support.')\n",
    "\n",
    "    if otflag is None:\n",
    "        # use dtw or fastdtw\n",
    "        if fast:\n",
    "            _, path = fastdtw(orgdata, tardata, dist=distance_func)\n",
    "            twf = np.array(path).T\n",
    "\n",
    "    return twf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melcd(array1, array2):\n",
    "    \"\"\"Calculate mel-cepstrum distortion\n",
    "    Calculate mel-cepstrum distortion between the arrays.\n",
    "    This function assumes the shapes of arrays are same.\n",
    "    Parameters\n",
    "    ----------\n",
    "    array1, array2 : array, shape (`T`, `dim`) or shape (`dim`)\n",
    "        Arrays of original and target.\n",
    "    Returns\n",
    "    -------\n",
    "    mcd : scala, number > 0\n",
    "        Scala of mel-cepstrum distortion\n",
    "    \"\"\"\n",
    "    if array1.shape != array2.shape:\n",
    "        raise ValueError(\n",
    "            \"The shapes of both arrays are different \\\n",
    "            : {} / {}\".format(array1.shape, array2.shape))\n",
    "\n",
    "    if array1.ndim == 2:\n",
    "        # array based melcd calculation\n",
    "        diff = array1 - array2\n",
    "        mcd = 10.0 / np.log(10) \\\n",
    "            * np.mean(np.sqrt(2.0 * np.sum(diff ** 2, axis=1)))\n",
    "    elif array1.ndim == 1:\n",
    "        diff = array1 - array2\n",
    "        mcd = 10.0 / np.log(10) * np.sqrt(2.0 * np.sum(diff ** 2))\n",
    "    else:\n",
    "        raise ValueError(\"Dimension mismatch\")\n",
    "\n",
    "    return mcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p225/p225_061.wav\n",
      "convert\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c5ff8b99cc70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mtarget_mcep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_wav_extract_mcep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./VCTK/wav48'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'convert'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0moriginal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./VCTK/wav48/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mconverted_mcep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_mcep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-cce0ef7abf06>\u001b[0m in \u001b[0;36mconversion\u001b[0;34m(original_wav, speaker)\u001b[0m\n\u001b[1;32m     30\u001b[0m                                                                \u001b[0mvalid_quantize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                                                                \u001b[0mohe_speaker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                                                                T=16126)\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mrecon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_reconstructions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mmu_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrecon\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m128\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/pytorch_vq_vae_audio/pytorch_vq_vae_audio/wavenet_vocoder/wavenet.py\u001b[0m in \u001b[0;36mincremental_forward\u001b[0;34m(self, initial_input, c, g, T, test_inputs, tqdm, softmax, quantize, log_scale_min)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0mskips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincremental_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0mskips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mskips\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mskips\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/pytorch_vq_vae_audio/pytorch_vq_vae_audio/wavenet_vocoder/modules.py\u001b[0m in \u001b[0;36mincremental_forward\u001b[0;34m(self, x, c, g)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mincremental_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_incremental\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/pytorch_vq_vae_audio/pytorch_vq_vae_audio/wavenet_vocoder/modules.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, x, c, g, is_incremental)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_conv1x1_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1x1g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_incremental\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mga\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplitdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplitdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mga\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path_dir = './'\n",
    "stride = 10\n",
    "now = stride\n",
    "end = 500\n",
    "\n",
    "conversion_list = np.array([])\n",
    "conversion_list = np.load('./2_speaker/m_train_set.npy')\n",
    "\n",
    "# validate_conversion_list = np.array([])\n",
    "# validate_conversion_list = np.load('./2_speaker/m_test_set.npy')\n",
    "\n",
    "while (now <= end):\n",
    "    file = 'model_epoch' + str(now)\n",
    "    file_list = os.listdir(path_dir)\n",
    "    if file in file_list:\n",
    "        mcd_per_epochs = []\n",
    "        validate_mcd_per_epochs = []\n",
    "        if now != stride:\n",
    "            mcd_per_epochs = np.load('mean_of_mcd_per_epochs.npy' ).tolist()\n",
    "#             validate_mcd_per_epochs =  = np.load('validate_mean_of_mcd_per_epochs'+str(now)).tolist()\n",
    "        model.load_state_dict(torch.load(\"model_epoch\"+str(now), map_location=torch.device(device) ))\n",
    "        \n",
    "        mcds = []\n",
    "        for i in conversion_list:\n",
    "            print(i[0][0])\n",
    "            target_mcep = load_wav_extract_mcep('./VCTK/wav48', i[2][0])\n",
    "            print('convert')\n",
    "            original = conversion('./VCTK/wav48/'+i[0][0], i[1][0])\n",
    "            converted_mcep = extract_mcep(original[1])\n",
    "            \n",
    "            mcds.append = mcd_cal(converted_mcep, target_mcep)\n",
    "        \n",
    "        mcd_per_epochs.append(np.mean(mcds))\n",
    "#         validate_mcd_per_epochs.append(mcd_cal(validate_conversion_list))\n",
    "            \n",
    "        now+=stride\n",
    "        np.save('mean_of_mcd_per_epochs.npy', np.array(mcd_per_epochs))\n",
    "#         np.save('validate_mean_of_mcd_per_epochs'+str(now), np.array(validate_mcd_per_epochs))\n",
    "    else:\n",
    "        print('sleep 60')\n",
    "        time.sleep(60)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
