{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [VQ-VAE](https://arxiv.org/abs/1711.00937) for audio in PyTorch\n",
    "\n",
    "This notebook is based on \n",
    "https://github.com/zalandoresearch/pytorch-vq-vae\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Variational Auto Encoders (VAEs) can be thought of as what all but the last layer of a neural network is doing, namely feature extraction or seperating out the data. Thus given some data we can think of using a neural network for representation generation. \n",
    "\n",
    "Recall that the goal of a generative model is to estimate the probability distribution of high dimensional data such as images, videos, audio or even text by learning the underlying structure in the data as well as the dependencies between the different elements of the data. This is very useful since we can then use this representation to generate new data with similar properties. This way we can also learn useful features from the data in an unsupervised fashion.\n",
    "\n",
    "The VQ-VAE uses a discrete latent representation mostly because many important real-world objects are discrete. For example in images we might have categories like \"Cat\", \"Car\", etc. and it might not make sense to interpolate between these categories. Discrete representations are also easier to model since each category has a single value whereas if we had a continous latent space then we will need to normalize this density function and learn the dependencies between the different variables which could be very complex.\n",
    "\n",
    "### Code\n",
    "\n",
    "I have followed the code from the TensorFlow implementation by the author which you can find here [vqvae.py](https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/vqvae.py) and [vqvae_example.ipynb](https://github.com/deepmind/sonnet/blob/master/sonnet/examples/vqvae_example.ipynb). \n",
    "\n",
    "Another PyTorch implementation is found at [pytorch-vqvae](https://github.com/ritheshkumar95/pytorch-vqvae).\n",
    "\n",
    "\n",
    "## Basic Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining a latent embedding space of dimension `[K, D]` where `K` are the number of embeddings and `D` is the dimensionality of each latent embeddng vector $e_i$.\n",
    "\n",
    "The model will take in batches of waveforms, of size 16126 for our example, and pass it through a ConvNet encoder producing some output, where we make sure the channels are the same as the dimensionality of the latent embedding vectors. To calculate the discrete latent variable we find the nearest embedding vector and output it's index. \n",
    "\n",
    "The input to the decoder is the embedding vector corresponding to the index which is passed through the decoder to produce the reconstructed audio. \n",
    "\n",
    "Since the nearest neighbour lookup has no real gradient in the backward pass we simply pass the gradients from the decoder to the encoder  unaltered. The intuition is that since the output representation of the encoder and the input to the decoder share the same `D` channel dimensional space, the gradients contain useful information for how the encoder has to change its output to lower the reconstruction loss.\n",
    "\n",
    "## Loss\n",
    "\n",
    "The total loss is composed of three components:\n",
    "\n",
    "1. reconstruction loss which optimizes the decoder and encoder\n",
    "1. due to the fact that gradients bypass the embedding, we use a dictionary learning algorithm  which uses an $l_2$  error to move the embedding vectors $e_i$ towards the encoder output\n",
    "1. also since the volume of the embedding space is dimensionless, it can grow arbirtarily if the embeddings $e_i$ do not train as fast as  the encoder parameters, and thus we add a commitment loss to make sure that the encoder commits to an embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import librosa\n",
    "import random\n",
    "from wavenet_vocoder.wavenet import WaveNet\n",
    "from wavenet_vocoder.wavenet import receptive_field_size\n",
    "#from vq import VectorQuantizerEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "args = easydict.EasyDict({\n",
    "    \"batch\": 1,\n",
    "    \"epochs\": 500,\n",
    "    \"training_data\": './2_speaker/vctk_train.txt',\n",
    "    \"test_data\": './2_speaker/vctk_test.txt',\n",
    "#    \"training_data\": './vctk_train.txt',\n",
    "#    \"test_data\": './vctk_test.txt',\n",
    "#    \"out\": \"result\",\n",
    "#    \"resume\": False,\n",
    "    \"load\": 0,\n",
    "    \"load_mid\" : 0,\n",
    "    \"seed\": 123456789 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "#torch.cuda.set_device(0)\n",
    "device\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.training_data, 'r') as f:\n",
    "    data = f.read()\n",
    "file = data.splitlines()\n",
    "speaker_dic = {}\n",
    "number_of_speakers = 0\n",
    "for i in range (0, len(file)):\n",
    "    if (file[i].split('/')[0] in speaker_dic):\n",
    "        continue\n",
    "    else :\n",
    "        speaker_dic[file[i].split('/')[0]] = number_of_speakers\n",
    "        number_of_speakers+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: check that weight gets updated\n",
    "class VectorQuantizerEMA(nn.Module):\n",
    "    \"\"\"We will also implement a slightly modified version  which will use exponential moving averages\n",
    "    to update the embedding vectors instead of an auxillary loss.\n",
    "    This has the advantage that the embedding updates are independent of the choice of optimizer \n",
    "    for the encoder, decoder and other parts of the architecture.\n",
    "    For most experiments the EMA version trains faster than the non-EMA version.\"\"\"\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay, epsilon=1e-5):\n",
    "        super(VectorQuantizerEMA, self).__init__()\n",
    "        \n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._num_embeddings = num_embeddings\n",
    "        \n",
    "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
    "        #self._embedding.weight.data.normal_()\n",
    "        self._embedding.weight.data.uniform_(-1./512, 1./512)\n",
    "#        self._embedding.weight.data = torch.Tensor([0])\n",
    "        #self._embedding.weight.data = torch.Tensor(np.zeros(()))\n",
    "        self._commitment_cost = commitment_cost\n",
    "        \n",
    "        self.register_buffer('_ema_cluster_size', torch.zeros(num_embeddings))\n",
    "        self._ema_w = nn.Parameter(torch.Tensor(num_embeddings, self._embedding_dim))\n",
    "        self._ema_w.data.normal_()\n",
    "        \n",
    "        self._decay = decay\n",
    "        self._epsilon = epsilon\n",
    "    '''\n",
    "    def forward(self, inputs):\n",
    "        # convert inputs from BCL -> BLC\n",
    "        inputs = inputs.permute(0, 2, 1).contiguous()\n",
    "        input_shape = inputs.shape\n",
    "        # Flatten input\n",
    "        flat_input = inputs.view(-1, self._embedding_dim)     #[BL, C]\n",
    "        if (self._embedding.weight.data == 0).all():\n",
    "            self._embedding.weight.data = flat_input[-self._num_embeddings:].detach()\n",
    "        # Calculate distances\n",
    "\n",
    "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) \n",
    "                    + torch.sum(self._embedding.weight**2, dim=1)\n",
    "                    - 2 * torch.matmul(flat_input, self._embedding.weight.t())) #[BL, num_embeddings]\n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1) #[BL, 1]\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings).to(device)# [BL, num_embeddings]\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "        #print(encodings.shape) [250, 512]\n",
    "        # Use EMA to update the embedding vectors\n",
    "        if self.training:\n",
    "            self._ema_cluster_size = self._ema_cluster_size * self._decay + \\\n",
    "                                     (1 - self._decay) * torch.sum(encodings, 0)\n",
    "            #print(self._ema_cluster_size.shape) [512]\n",
    "            n = torch.sum(self._ema_cluster_size)\n",
    "            self._ema_cluster_size = (\n",
    "                (self._ema_cluster_size + self._epsilon)\n",
    "                / (n + self._num_embeddings * self._epsilon) * n)\n",
    "            \n",
    "            dw = torch.matmul(encodings.t(), flat_input)\n",
    "            self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw)\n",
    "            \n",
    "            self._embedding.weight = nn.Parameter(self._ema_w / self._ema_cluster_size.unsqueeze(1))\n",
    "        \n",
    "        # Quantize and unflatten\n",
    "        #encodings.shape = [BL, num_embeddings] , weight.shape=[num_embeddings, C]\n",
    "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
    "\n",
    "        \n",
    "        # Loss\n",
    "        e_latent_loss = torch.mean((quantized.detach() - inputs)**2)\n",
    "        q_latent_loss = torch.mean((quantized - inputs.detach())**2)\n",
    "#        print(q_latent_loss.item(), 0.25 * e_latent_loss.item())\n",
    "        loss = q_latent_loss + self._commitment_cost * e_latent_loss\n",
    "        \n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "        # convert quantized from BLC -> BCL\n",
    "        return loss, quantized.permute(0, 2, 1).contiguous(), perplexity\n",
    "    '''\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # convert inputs from BCL -> BLC\n",
    "        inputs = inputs.permute(0, 2, 1).contiguous()\n",
    "        input_shape = inputs.shape\n",
    "        # Flatten input\n",
    "        flat_input = inputs.view(-1, self._embedding_dim)     #[BL, C]\n",
    "        # Calculate distances\n",
    "        \n",
    "        distances = torch.norm(flat_input.unsqueeze(1) - self._embedding.weight, dim=2, p=2)\n",
    " #       distances = (torch.sum(flat_input**2, dim=1, keepdim=True) \n",
    " #                   + torch.sum(self._embedding.weight**2, dim=1)\n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1) #[BL, 1]\n",
    "        print(encoding_indices.unsqueeze(1))\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings).to(device)# [BL, num_embeddings]\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "        #print(encodings.shape) [250, 512]\n",
    "\n",
    "#         # Use EMA to update the embedding vectors\n",
    "#         if self.training:\n",
    "#             self._ema_cluster_size = self._ema_cluster_size * self._decay + \\\n",
    "#                                      (1 - self._decay) * torch.sum(encodings, 0)\n",
    "#             #print(self._ema_cluster_size.shape) [512]\n",
    "#             n = torch.sum(self._ema_cluster_size)\n",
    "#             self._ema_cluster_size = (\n",
    "#                 (self._ema_cluster_size + self._epsilon)\n",
    "#                 / (n + self._num_embeddings * self._epsilon) * n)\n",
    "            \n",
    "#             dw = torch.matmul(encodings.t(), flat_input)\n",
    "#             self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw)\n",
    "            \n",
    "#             self._embedding.weight = nn.Parameter(self._ema_w / self._ema_cluster_size.unsqueeze(1))\n",
    "\n",
    "        # Quantize and unflatten\n",
    "        #encodings.shape = [BL, num_embeddings] , weight.shape=[num_embeddings, C]\n",
    "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
    "        # Loss\n",
    "        e_latent_loss = torch.mean((quantized.detach() - inputs)**2)\n",
    "        q_latent_loss = torch.mean((quantized - inputs.detach())**2)\n",
    "#        print(q_latent_loss.item(), 0.25 * e_latent_loss.item())\n",
    "        loss = q_latent_loss + self._commitment_cost * e_latent_loss\n",
    "        \n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "        # same as torch.exp( entropy loss )\n",
    "        \n",
    "        # convert quantized from BLC -> BCL\n",
    "        return loss, quantized.permute(0, 2, 1).contiguous(), perplexity\n",
    "#    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder & Decoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Audio encoder\n",
    "    The vq-vae paper says that the encoder has 6 strided convolutions with stride 2 and window-size 4.\n",
    "    The number of channels and a nonlinearity is not specified in the paper. \n",
    "    I tried using ReLU, it didn't work.\n",
    "    Now I try using tanh, hoping that this will keep my encoded values within the neighborhood of 0,\n",
    "    so they do not drift too far away from encoding vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    c1 = nn.Conv1d(in_channels=256, out_channels = 512, stride=2,kernel_size=4,padding=0)\n",
    "    c2 = nn.Conv1d(in_channels=512, out_channels = 512, stride=2,kernel_size=4,padding=0)\n",
    "    c3 = nn.Conv1d(in_channels=512, out_channels = 512, stride=2,kernel_size=4,padding=0)\n",
    "    c4 = nn.Conv1d(in_channels=512, out_channels = 512, stride=2,kernel_size=4,padding=0)\n",
    "    c5 = nn.Conv1d(in_channels=512, out_channels = 512, stride=2,kernel_size=4,padding=0)\n",
    "    c6 = nn.Conv1d(in_channels=512, out_channels = 64, stride=2,kernel_size=4,padding=0)\n",
    "\n",
    "\n",
    "    def __init__(self, encoding_channels, in_channels=256):\n",
    "        super(Encoder,self).__init__()\n",
    "        self._num_layers = 2 * len(encoding_channels)\n",
    "        self._layers = nn.ModuleList()\n",
    "        nn.init.xavier_uniform_(self.c1.weight)\n",
    "        nn.init.xavier_uniform_(self.c2.weight)\n",
    "        nn.init.xavier_uniform_(self.c3.weight)\n",
    "        nn.init.xavier_uniform_(self.c4.weight)\n",
    "        nn.init.xavier_uniform_(self.c5.weight)\n",
    "        nn.init.xavier_uniform_(self.c6.weight)\n",
    "        \n",
    "        self._layers.append(self.c1)\n",
    "        self._layers.append(nn.Tanh())\n",
    "        self._layers.append(self.c2)\n",
    "        self._layers.append(nn.Tanh())\n",
    "        self._layers.append(self.c3)\n",
    "        self._layers.append(nn.Tanh())\n",
    "        self._layers.append(self.c4)\n",
    "        self._layers.append(nn.Tanh())\n",
    "        self._layers.append(self.c5)\n",
    "        self._layers.append(nn.Tanh())\n",
    "        self._layers.append(self.c6)\n",
    "        self._layers.append(nn.Tanh())\n",
    "        \n",
    "#         for out_channels in encoding_channels:\n",
    "#             self._layers.append(nn.Conv1d(in_channels=in_channels,\n",
    "#                                     out_channels=out_channels,\n",
    "#                                     stride=2,\n",
    "#                                     kernel_size=4,\n",
    "#                                     padding=0, \n",
    "#                                         ))\n",
    "#             self._layers.append(nn.Tanh())\n",
    "#             in_channels = out_channels\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(self._num_layers):\n",
    "            x = self._layers[i](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoding_channels,\n",
    "                 num_embeddings, \n",
    "                 embedding_dim,\n",
    "                 commitment_cost, \n",
    "                 layers,\n",
    "                 stacks,\n",
    "                 kernel_size,\n",
    "                 decay=0):\n",
    "        super(Model, self).__init__()       \n",
    "        self._encoder = Encoder(encoding_channels=encoding_channels)\n",
    "        #I tried adding batch normalization here, because:\n",
    "        #the distribution of encoded values needs to be similar to the distribution of embedding vectors\n",
    "        #otherwise we'll see \"posterior collapse\": all values will be assigned to the same embedding vector,\n",
    "        #and stay that way (because vectors which do not get assigned anything do not get updated).\n",
    "        #Batch normalization is a way to fix that. But it didn't work: model\n",
    "        #reproduced voice correctly, but the words were completely wrong.\n",
    "        #self._batch_norm = nn.BatchNorm1d(1)\n",
    "        if decay > 0.0:\n",
    "#             self._vq_vae = EMVectorQuantizerEMA(num_embeddings, embedding_dim, \n",
    "#                                               commitment_cost, decay, 100)\n",
    "            self._vq_vae = VectorQuantizerEMA(num_embeddings, embedding_dim, \n",
    "                                               commitment_cost, decay)\n",
    "\n",
    "        else:\n",
    "            self._vq_vae = VectorQuantizer(num_embeddings, embedding_dim,\n",
    "                                           commitment_cost)\n",
    "        self._decoder = WaveNet(device, out_channels=256, #dimension of ohe mu-quantized signal\n",
    "                                layers=layers, #like in original WaveNet\n",
    "                                stacks=stacks,\n",
    "                                residual_channels=512,\n",
    "                                gate_channels=512,\n",
    "                                skip_out_channels=512,\n",
    "                                kernel_size=kernel_size, \n",
    "                                dropout=1 - 0.95,\n",
    "                                cin_channels=embedding_dim, #local conditioning channels - on encoder output\n",
    "                                gin_channels=number_of_speakers, #global conditioning channels - on speaker_id\n",
    "                                n_speakers=number_of_speakers,\n",
    "                                weight_normalization=False, \n",
    "                                upsample_conditional_features=True, \n",
    "                                decoding_channels=encoding_channels[::-1],\n",
    "                                use_speaker_embedding=False\n",
    "                               )\n",
    "        self.recon_loss = torch.nn.CrossEntropyLoss()\n",
    "        self.receptive_field = receptive_field_size(total_layers=layers, num_cycles=stacks, kernel_size=kernel_size)\n",
    "#        self.mean = None\n",
    "#        self.std = None\n",
    "    def forward(self, x):\n",
    "        audio, target, speaker_id = x\n",
    "        assert len(audio.shape) == 3 # B x C x L \n",
    "        assert audio.shape[1] == 256\n",
    "        z = self._encoder(audio)\n",
    "        #normalize output - subtract mean, divide by standard deviation\n",
    "        #without this, perplexity goes to 1 almost instantly\n",
    "#         if self.mean is None:\n",
    "#             self.mean = z.mean().detach()\n",
    "#         if self.std is None:\n",
    "#              self.std = z.std().detach()\n",
    "#        z = z - self.mean\n",
    "#        z = z / self.std\n",
    "        vq_loss, quantized, perplexity = self._vq_vae(z)\n",
    "#        assert z.shape == quantized.shape\n",
    "#        print(\"audio.shape\", audio.shape)\n",
    "#        print(\"quantized.shape\", quantized.shape)\n",
    "        x_recon = self._decoder(audio, quantized, speaker_id, softmax=False)\n",
    "        x_recon = x_recon[:, :, self.receptive_field:-1]\n",
    "        recon_loss_value = self.recon_loss(x_recon, target[:, 1:])\n",
    "        loss = recon_loss_value + vq_loss\n",
    "        \n",
    "        return loss, recon_loss_value, x_recon, perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_updates = 39818\n",
    "#vector quantizer parameters:\n",
    "embedding_dim = 64 #dimension of each vector\n",
    "encoding_channels = [512,512,512,512,512,embedding_dim]\n",
    "num_embeddings = 512 #number of vectors\n",
    "commitment_cost = 0.25\n",
    "\n",
    "#wavenet parameters:\n",
    "kernel_size=2\n",
    "total_layers=30\n",
    "num_cycles=3\n",
    "\n",
    "\n",
    "decay = 0.99\n",
    "#decay = 0\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3070\n"
     ]
    }
   ],
   "source": [
    "receptive_field = receptive_field_size(total_layers=total_layers, num_cycles=num_cycles, kernel_size=kernel_size)\n",
    "print(receptive_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(num_embeddings=num_embeddings,\n",
    "              encoding_channels=encoding_channels,\n",
    "              embedding_dim=embedding_dim, \n",
    "              commitment_cost=commitment_cost, \n",
    "              layers=total_layers,\n",
    "              stacks=num_cycles,\n",
    "              kernel_size=kernel_size,\n",
    "              decay=decay).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1, amsgrad=False)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer,\n",
    "                                              lr_lambda=lambda epoch: 1e-3 if epoch == 0 else  (optimizer.param_groups[0]['lr'] - (1e-3 - 1e-6)/500) if epoch <= 500 else optimizer.param_groups[0]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingSet(Dataset):\n",
    "    # VCTK-Corpus Training data set\n",
    "\n",
    "    def __init__(self, num_speakers,\n",
    "                 receptive_field,\n",
    "                 segment_length=16126,\n",
    "                 chunk_size=1000,\n",
    "                 classes=256):\n",
    "        \n",
    "        self.x_list = self.read_files(args.training_data)\n",
    "        self.classes = 256\n",
    "        self.segment_length = segment_length\n",
    "        self.chunk_size = chunk_size\n",
    "        self.classes = classes\n",
    "        self.receptive_field = receptive_field\n",
    "        self.cached_pt = 0\n",
    "        self.num_speakers = num_speakers\n",
    "\n",
    "    def read_files(self, filename):\n",
    "        print(\"training data from \" + args.training_data)\n",
    "        with open(filename) as file:\n",
    "            files = file.readlines()\n",
    "        return [f.strip() for f in files]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            audio, sr = librosa.load('./VCTK/wav48/'+self.x_list[index])\n",
    "        except Exception as e:\n",
    "            print(e, audiofile)\n",
    "        if sr != 22050:\n",
    "            raise ValueError(\"{} SR of {} not equal to 22050\".format(sr, audiofile))\n",
    "            \n",
    "        audio = librosa.util.normalize(audio) #divide max(abs(audio))\n",
    "        audio = self.quantize_data(audio, self.classes)\n",
    "            \n",
    "        while audio.shape[0] < self.segment_length:\n",
    "            index += 1\n",
    "            audio, speaker_id = librosa.load('./VCTK/wav48/'+self.x_list[index])\n",
    "            \n",
    "        max_audio_start = audio.shape[0] - self.segment_length\n",
    "        audio_start = random.randint(0, max_audio_start)\n",
    "        audio = audio[audio_start:audio_start+self.segment_length]\n",
    "        \n",
    "                #divide into input and target\n",
    "        audio = torch.from_numpy(audio)\n",
    "        ohe_audio = torch.FloatTensor(self.classes, self.segment_length).zero_()\n",
    "        ohe_audio.scatter_(0, audio.unsqueeze(0), 1.)\n",
    "        target = audio[self.receptive_field:]\n",
    "            \n",
    "        speaker_index = speaker_dic[self.x_list[index].split('/')[0]]\n",
    "        speaker_id = torch.from_numpy(np.array(speaker_index)).unsqueeze(0).unsqueeze(0)\n",
    "        ohe_speaker = torch.FloatTensor(self.num_speakers, 1).zero_()\n",
    "        ohe_speaker.scatter_(0, speaker_id, 1.)\n",
    "        \n",
    "        return ohe_audio, target, ohe_speaker\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_list)\n",
    "    \n",
    "    def quantize_data(self, data, classes):\n",
    "        mu_x = self.mu_law_encode(data, classes)\n",
    "        bins = np.linspace(-1, 1, classes)\n",
    "        quantized = np.digitize(mu_x, bins) - 1\n",
    "        return quantized\n",
    "\n",
    "    def mu_law_encode(self, data, mu):\n",
    "        mu_x = np.sign(data) * np.log(1 + mu * np.abs(data)) / np.log(mu + 1)\n",
    "        return mu_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestSet(Dataset):\n",
    "    # VCTK-Corpus Test data set\n",
    "\n",
    "\n",
    "    def __init__(self, num_speakers,\n",
    "                 receptive_field,\n",
    "                 segment_length=16126,\n",
    "                 chunk_size=1000,\n",
    "                 classes=256):\n",
    "        \n",
    "        \n",
    "        self.x_list = self.read_files(args.test_data)\n",
    "        self.classes = 256\n",
    "        self.segment_length = segment_length\n",
    "        self.chunk_size = chunk_size\n",
    "        self.classes = classes\n",
    "        self.receptive_field = receptive_field\n",
    "        self.cached_pt = 0\n",
    "        self.num_speakers = num_speakers\n",
    "\n",
    "\n",
    "    def read_files(self, filename):\n",
    "        print(\"training data from \" + args.test_data)\n",
    "        with open(filename) as file:\n",
    "            files = file.readlines()\n",
    "        return [f.strip() for f in files]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            audio, sr = librosa.load('./VCTK/wav48/'+self.x_list[index])\n",
    "        except Exception as e:\n",
    "            print(e, audiofile)\n",
    "        if sr != 22050:\n",
    "            raise ValueError(\"{} SR of {} not equal to 22050\".format(sr, audiofile))\n",
    "        \n",
    "        audio = librosa.util.normalize(audio) #divide max(abs(audio))\n",
    "        audio = self.quantize_data(audio, self.classes)\n",
    "            \n",
    "        while audio.shape[0] < self.segment_length:\n",
    "            index += 1\n",
    "            audio, speaker_id = librosa.load('./VCTK/wav48/'+self.x_list[index])\n",
    "            \n",
    "        max_audio_start = audio.shape[0] - self.segment_length\n",
    "        audio_start = random.randint(0, max_audio_start)\n",
    "        audio = audio[audio_start:audio_start+self.segment_length]\n",
    "        \n",
    "                #divide into input and target\n",
    "        audio = torch.from_numpy(audio)\n",
    "        ohe_audio = torch.FloatTensor(self.classes, self.segment_length).zero_()\n",
    "        ohe_audio.scatter_(0, audio.unsqueeze(0), 1.)\n",
    "        target = audio[self.receptive_field:]\n",
    "            \n",
    "        speaker_index = speaker_dic[self.x_list[index].split('/')[0]]\n",
    "        speaker_id = torch.from_numpy(np.array(speaker_index)).unsqueeze(0).unsqueeze(0)\n",
    "        ohe_speaker = torch.FloatTensor(self.num_speakers, 1).zero_()\n",
    "        ohe_speaker.scatter_(0, speaker_id, 1.)\n",
    "        \n",
    "        return ohe_audio, target, ohe_speaker\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_list)\n",
    "        \n",
    "    def quantize_data(self, data, classes):\n",
    "        mu_x = self.mu_law_encode(data, classes)\n",
    "        bins = np.linspace(-1, 1, classes)\n",
    "        quantized = np.digitize(mu_x, bins) - 1\n",
    "        return quantized\n",
    "\n",
    "    def mu_law_encode(self, data, mu):\n",
    "        mu_x = np.sign(data) * np.log(1 + mu * np.abs(data)) / np.log(mu + 1)\n",
    "        return mu_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data from ./2_speaker/vctk_train.txt\n",
      "training data from ./2_speaker/vctk_test.txt\n"
     ]
    }
   ],
   "source": [
    "trainset = TrainingSet(number_of_speakers, receptive_field=receptive_field)\n",
    "testset = TestSet(number_of_speakers, receptive_field=receptive_field)\n",
    "\n",
    "\n",
    "training_loader = DataLoader(dataset = trainset,\n",
    "                           batch_size=batch_size,\n",
    "                           shuffle=True, \n",
    "                           num_workers=1)\n",
    "\n",
    "\n",
    "validation_loader = DataLoader(dataset = testset,\n",
    "                           batch_size=batch_size,\n",
    "                           shuffle=True, \n",
    "                           num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res_recon_error = []\n",
    "train_res_perplexity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    global train_res_recon_error\n",
    "    global train_res_perplexity\n",
    "    train_total_loss = []\n",
    "    train_recon_error = []\n",
    "    train_perplexity = []\n",
    "    # with open(\"errors\", \"rb\") as file:\n",
    "    #     train_res_recon_error, train_res_perplexity = pickle.load(file)\n",
    "# num_epochs = 1\n",
    "# for epoch in range(num_epochs):\n",
    "    iterator = iter(training_loader)\n",
    "#     datas0 = []\n",
    "#     datas1 = []\n",
    "#     datas2 = []\n",
    "    for i, data_train in enumerate(iterator):\n",
    "        data_train = [data_train[0].to(device),\n",
    "                     data_train[1].to(device),\n",
    "                     data_train[2].to(device)\n",
    "                     ]\n",
    "\n",
    "#         datas0.append(data_train[0])\n",
    "#         datas1.append(data_train[1])\n",
    "#         datas2.append(data_train[2])\n",
    "#         if (i+1) % batch_size == 0:\n",
    "#             data = [torch.cat(datas0).to(device),\n",
    "#                    torch.cat(datas1).to(device),\n",
    "#                    torch.cat(datas2).to(device)]\n",
    "        optimizer.zero_grad()\n",
    "        loss, recon_error, data_recon, perplexity = model(data_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_total_loss.append(loss.item())\n",
    "        train_recon_error.append(recon_error.item())\n",
    "        train_perplexity.append(perplexity.item())\n",
    "\n",
    "        if (i+1) % (10 * batch_size) == 0:\n",
    "            print('%d iterations' % (i+1))\n",
    "            print('recon_error: %.3f' % np.mean(train_recon_error[-100:]))\n",
    "            print('perplexity: %.3f' % np.mean(train_perplexity[-100:]))\n",
    "            print()\n",
    "    train_res_recon_error.extend(train_recon_error)\n",
    "    train_res_perplexity.extend(train_perplexity)\n",
    "    return np.mean(train_total_loss), np.mean(train_res_recon_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_total_loss = []\n",
    "        test_res_recon_error = []\n",
    "        # with open(\"errors\", \"rb\") as file:\n",
    "        #     train_res_recon_error, train_res_perplexity = pickle.load(file)\n",
    "    # num_epochs = 1\n",
    "    # for epoch in range(num_epochs):\n",
    "        iterator = iter(validation_loader)\n",
    "    #     datas0 = []\n",
    "    #     datas1 = []\n",
    "    #     datas2 = []\n",
    "        for i, data_test in enumerate(iterator):\n",
    "            data_test = [data_test[0].to(device),\n",
    "                         data_test[1].to(device),\n",
    "                         data_test[2].to(device)]\n",
    "            \n",
    "            loss, recon_error, data_recon, perplexity = model(data_test)\n",
    "\n",
    "            test_total_loss.append(loss.item())\n",
    "            test_res_recon_error.append(recon_error.item())\n",
    "\n",
    "            if (i+1) % (10 * batch_size) == 0:\n",
    "                print('%d iterations' % (i+1))\n",
    "                print('recon_error: %.3f' % np.mean(test_res_recon_error[-100:]))\n",
    "                print()\n",
    "    return np.mean(test_total_loss), np.mean(test_res_recon_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epochs ==> training\n",
      "tensor([[[430]],\n",
      "\n",
      "        [[153]],\n",
      "\n",
      "        [[ 80]],\n",
      "\n",
      "        [[484]],\n",
      "\n",
      "        [[201]],\n",
      "\n",
      "        [[349]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[ 20]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[186]],\n",
      "\n",
      "        [[189]],\n",
      "\n",
      "        [[189]],\n",
      "\n",
      "        [[253]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[113]],\n",
      "\n",
      "        [[405]],\n",
      "\n",
      "        [[346]],\n",
      "\n",
      "        [[ 63]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[239]],\n",
      "\n",
      "        [[ 20]],\n",
      "\n",
      "        [[308]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[321]],\n",
      "\n",
      "        [[275]],\n",
      "\n",
      "        [[436]],\n",
      "\n",
      "        [[135]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[447]],\n",
      "\n",
      "        [[466]],\n",
      "\n",
      "        [[168]],\n",
      "\n",
      "        [[405]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[130]],\n",
      "\n",
      "        [[349]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[396]],\n",
      "\n",
      "        [[264]],\n",
      "\n",
      "        [[147]],\n",
      "\n",
      "        [[466]],\n",
      "\n",
      "        [[436]],\n",
      "\n",
      "        [[370]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[239]],\n",
      "\n",
      "        [[105]],\n",
      "\n",
      "        [[138]],\n",
      "\n",
      "        [[115]],\n",
      "\n",
      "        [[351]],\n",
      "\n",
      "        [[264]],\n",
      "\n",
      "        [[480]],\n",
      "\n",
      "        [[139]],\n",
      "\n",
      "        [[447]],\n",
      "\n",
      "        [[335]],\n",
      "\n",
      "        [[ 20]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[151]],\n",
      "\n",
      "        [[ 44]],\n",
      "\n",
      "        [[ 15]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[198]],\n",
      "\n",
      "        [[146]],\n",
      "\n",
      "        [[113]],\n",
      "\n",
      "        [[339]],\n",
      "\n",
      "        [[398]],\n",
      "\n",
      "        [[239]],\n",
      "\n",
      "        [[264]],\n",
      "\n",
      "        [[275]],\n",
      "\n",
      "        [[398]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[316]],\n",
      "\n",
      "        [[ 15]],\n",
      "\n",
      "        [[153]],\n",
      "\n",
      "        [[436]],\n",
      "\n",
      "        [[370]],\n",
      "\n",
      "        [[186]],\n",
      "\n",
      "        [[ 15]],\n",
      "\n",
      "        [[138]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[141]],\n",
      "\n",
      "        [[153]],\n",
      "\n",
      "        [[448]],\n",
      "\n",
      "        [[253]],\n",
      "\n",
      "        [[101]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[484]],\n",
      "\n",
      "        [[ 20]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[186]],\n",
      "\n",
      "        [[398]],\n",
      "\n",
      "        [[ 20]],\n",
      "\n",
      "        [[264]],\n",
      "\n",
      "        [[398]],\n",
      "\n",
      "        [[365]],\n",
      "\n",
      "        [[186]],\n",
      "\n",
      "        [[ 20]],\n",
      "\n",
      "        [[111]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[177]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[190]],\n",
      "\n",
      "        [[436]],\n",
      "\n",
      "        [[405]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[385]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[145]],\n",
      "\n",
      "        [[113]],\n",
      "\n",
      "        [[447]],\n",
      "\n",
      "        [[455]],\n",
      "\n",
      "        [[186]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[448]],\n",
      "\n",
      "        [[205]],\n",
      "\n",
      "        [[262]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[145]],\n",
      "\n",
      "        [[157]],\n",
      "\n",
      "        [[370]],\n",
      "\n",
      "        [[153]],\n",
      "\n",
      "        [[ 20]],\n",
      "\n",
      "        [[ 15]],\n",
      "\n",
      "        [[370]],\n",
      "\n",
      "        [[ 26]],\n",
      "\n",
      "        [[436]],\n",
      "\n",
      "        [[286]],\n",
      "\n",
      "        [[308]],\n",
      "\n",
      "        [[141]],\n",
      "\n",
      "        [[502]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[194]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[365]],\n",
      "\n",
      "        [[116]],\n",
      "\n",
      "        [[349]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[130]],\n",
      "\n",
      "        [[ 55]],\n",
      "\n",
      "        [[466]],\n",
      "\n",
      "        [[116]],\n",
      "\n",
      "        [[447]],\n",
      "\n",
      "        [[126]],\n",
      "\n",
      "        [[264]],\n",
      "\n",
      "        [[320]],\n",
      "\n",
      "        [[436]],\n",
      "\n",
      "        [[458]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[436]],\n",
      "\n",
      "        [[484]],\n",
      "\n",
      "        [[116]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[105]],\n",
      "\n",
      "        [[349]],\n",
      "\n",
      "        [[ 50]],\n",
      "\n",
      "        [[485]],\n",
      "\n",
      "        [[320]],\n",
      "\n",
      "        [[ 15]],\n",
      "\n",
      "        [[264]],\n",
      "\n",
      "        [[398]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[370]],\n",
      "\n",
      "        [[435]],\n",
      "\n",
      "        [[264]],\n",
      "\n",
      "        [[396]],\n",
      "\n",
      "        [[264]],\n",
      "\n",
      "        [[260]],\n",
      "\n",
      "        [[ 93]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[105]],\n",
      "\n",
      "        [[168]],\n",
      "\n",
      "        [[130]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[448]],\n",
      "\n",
      "        [[168]],\n",
      "\n",
      "        [[275]],\n",
      "\n",
      "        [[130]],\n",
      "\n",
      "        [[436]],\n",
      "\n",
      "        [[436]],\n",
      "\n",
      "        [[147]],\n",
      "\n",
      "        [[102]],\n",
      "\n",
      "        [[151]],\n",
      "\n",
      "        [[446]],\n",
      "\n",
      "        [[432]],\n",
      "\n",
      "        [[186]],\n",
      "\n",
      "        [[447]],\n",
      "\n",
      "        [[268]],\n",
      "\n",
      "        [[351]],\n",
      "\n",
      "        [[247]],\n",
      "\n",
      "        [[447]],\n",
      "\n",
      "        [[150]],\n",
      "\n",
      "        [[396]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[484]],\n",
      "\n",
      "        [[400]],\n",
      "\n",
      "        [[320]],\n",
      "\n",
      "        [[458]],\n",
      "\n",
      "        [[320]],\n",
      "\n",
      "        [[141]],\n",
      "\n",
      "        [[389]],\n",
      "\n",
      "        [[158]],\n",
      "\n",
      "        [[ 75]],\n",
      "\n",
      "        [[ 76]],\n",
      "\n",
      "        [[130]],\n",
      "\n",
      "        [[201]],\n",
      "\n",
      "        [[269]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[ 54]],\n",
      "\n",
      "        [[147]],\n",
      "\n",
      "        [[291]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[  4]],\n",
      "\n",
      "        [[153]],\n",
      "\n",
      "        [[113]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[157]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[370]],\n",
      "\n",
      "        [[467]],\n",
      "\n",
      "        [[396]],\n",
      "\n",
      "        [[335]],\n",
      "\n",
      "        [[105]],\n",
      "\n",
      "        [[247]],\n",
      "\n",
      "        [[291]],\n",
      "\n",
      "        [[145]],\n",
      "\n",
      "        [[447]],\n",
      "\n",
      "        [[157]],\n",
      "\n",
      "        [[168]],\n",
      "\n",
      "        [[186]],\n",
      "\n",
      "        [[247]],\n",
      "\n",
      "        [[320]],\n",
      "\n",
      "        [[ 15]],\n",
      "\n",
      "        [[419]],\n",
      "\n",
      "        [[145]],\n",
      "\n",
      "        [[308]],\n",
      "\n",
      "        [[269]],\n",
      "\n",
      "        [[391]],\n",
      "\n",
      "        [[ 74]],\n",
      "\n",
      "        [[125]],\n",
      "\n",
      "        [[186]],\n",
      "\n",
      "        [[205]],\n",
      "\n",
      "        [[428]],\n",
      "\n",
      "        [[ 80]],\n",
      "\n",
      "        [[145]],\n",
      "\n",
      "        [[113]],\n",
      "\n",
      "        [[342]],\n",
      "\n",
      "        [[113]]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6b84859b18ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" epochs ==> training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreconstruction_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mtraining_total_loss_per_epochs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mtraining_reconstruction_errors_per_epochs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstruction_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-25cc2e033517>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_recon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtrain_total_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = args.epochs\n",
    "training_total_loss_per_epochs = []\n",
    "training_reconstruction_errors_per_epochs = []\n",
    "validation_total_loss_per_epochs = []\n",
    "validation_reconstruction_errors_per_epochs = []\n",
    "\n",
    "lrs = []\n",
    "\n",
    "if (args.load != 0):\n",
    "    model.load_state_dict(torch.load(\"model_epoch\"+str(args.load)))\n",
    "    optimizer.load_state_dict(torch.load(\"optim_epoch\"+str(args.load)))\n",
    "    training_total_loss_per_epochs = np.load('training_total_loss_per_epochs'+str(args.load)+'.npy').tolist()\n",
    "    training_reconstruction_errors_per_epochs = np.load('training_reconstruction_errors_per_epochs'+str(args.load)+'.npy').tolist()\n",
    "    validation_total_loss_per_epochs = np.load('validation_total_loss_per_epochs'+str(args.load)+'.npy').tolist()\n",
    "    validation_reconstruction_errors_per_epochs = np.load('validation_reconstruction_errors_per_epochs'+str(args.load)+'.npy').tolist()\n",
    "    lrs = np.load('lrs.npy')\n",
    "    \n",
    "    \n",
    "if (args.load_mid != 0 and args.load == 0):\n",
    "    model.load_state_dict(torch.load(\"model_epoch\"+str(args.load)))\n",
    "    optimizer.load_state_dict(torch.load(\"optim_epoch\"+str(args.load)))\n",
    "\n",
    "\n",
    "for i in range(1, epochs+1):\n",
    "    print(str(i)+\" epochs ==> training\")\n",
    "    total_loss, reconstruction_loss = train()\n",
    "    training_total_loss_per_epochs.append(total_loss)\n",
    "    training_reconstruction_errors_per_epochs.append(reconstruction_loss)\n",
    "    \n",
    "    print(str(i)+\" epochs ==> validation\")\n",
    "    total_loss, reconstruction_loss = validation()\n",
    "    validation_total_loss_per_epochs.append(total_loss)\n",
    "    validation_reconstruction_errors_per_epochs.append(reconstruction_loss)\n",
    "\n",
    "    \n",
    "    if (i % 5 == 0):\n",
    "        torch.save(model.state_dict(), \"model_epoch\"+str(i+args.load))\n",
    "        torch.save(optimizer.state_dict(), \"optim_epoch\"+str(i+args.load))\n",
    "        \n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr = param_group['lr']\n",
    "    lrs.append(lr)\n",
    "    np.save('lrs.npy', lrs)\n",
    "    np.save('training_total_loss_per_epochs'+str(args.epochs + args.load), np.array(training_total_loss_per_epochs))\n",
    "    np.save('training_reconstruction_errors_per_epochs'+str(args.epochs + args.load), np.array(training_reconstruction_errors_per_epochs))\n",
    "    np.save('validation_total_loss_per_epochs'+str(args.epochs + args.load), np.array(validation_total_loss_per_epochs))\n",
    "    np.save('validation_reconstruction_errors_per_epochs'+str(args.epochs + args.load), np.array(validation_reconstruction_errors_per_epochs))\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
