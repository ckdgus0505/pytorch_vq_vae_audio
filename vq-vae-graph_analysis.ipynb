{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import librosa\n",
    "from wavenet_vocoder.wavenet import WaveNet\n",
    "from wavenet_vocoder.wavenet import receptive_field_size\n",
    "#from vq import VectorQuantizerEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "args = easydict.EasyDict({\n",
    "    \"batch\": 1,\n",
    "    \"training_data\": './2_speaker/vctk_train.txt',\n",
    "    \"load\": 30,\n",
    "    \"seed\": 123456789 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "#torch.cuda.set_device(0)\n",
    "device\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.training_data, 'r') as f:\n",
    "    data = f.read()\n",
    "file = data.splitlines()\n",
    "speaker_dic = {}\n",
    "number_of_speakers = 0\n",
    "for i in range (0, len(file)):\n",
    "    if (file[i].split('/')[0] in speaker_dic):\n",
    "        continue\n",
    "    else :\n",
    "        speaker_dic[file[i].split('/')[0]] = number_of_speakers\n",
    "        number_of_speakers+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: check that weight gets updated\n",
    "class VectorQuantizerEMA(nn.Module):\n",
    "    \"\"\"We will also implement a slightly modified version  which will use exponential moving averages\n",
    "    to update the embedding vectors instead of an auxillary loss.\n",
    "    This has the advantage that the embedding updates are independent of the choice of optimizer \n",
    "    for the encoder, decoder and other parts of the architecture.\n",
    "    For most experiments the EMA version trains faster than the non-EMA version.\"\"\"\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay, epsilon=1e-5):\n",
    "        super(VectorQuantizerEMA, self).__init__()\n",
    "        \n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._num_embeddings = num_embeddings\n",
    "        \n",
    "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
    "#        nn.init.uniform_(self._embedding.weight)\n",
    "#         self._embedding.weight.data.normal_(0,0.001)\n",
    "#        self._embedding.weight.data.uniform_(-1,1)\n",
    "#        self._embedding.weight.data = torch.Tensor([0])\n",
    "        #self._embedding.weight.data = torch.Tensor(np.zeros(()))\n",
    "        self._commitment_cost = commitment_cost\n",
    "        \n",
    "        self.register_buffer('_ema_cluster_size', torch.zeros(num_embeddings))\n",
    "        self._ema_w = nn.Parameter(torch.Tensor(num_embeddings, self._embedding_dim))\n",
    "        self._ema_w.data.normal_()\n",
    "        \n",
    "        self._decay = decay\n",
    "        self._epsilon = epsilon\n",
    "    '''\n",
    "    def forward(self, inputs):\n",
    "        # convert inputs from BCL -> BLC\n",
    "        inputs = inputs.permute(0, 2, 1).contiguous()\n",
    "        input_shape = inputs.shape\n",
    "        # Flatten input\n",
    "        flat_input = inputs.view(-1, self._embedding_dim)     #[BL, C]\n",
    "        if (self._embedding.weight.data == 0).all():\n",
    "            self._embedding.weight.data = flat_input[-self._num_embeddings:].detach()\n",
    "        # Calculate distances\n",
    "\n",
    "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) \n",
    "                    + torch.sum(self._embedding.weight**2, dim=1)\n",
    "                    - 2 * torch.matmul(flat_input, self._embedding.weight.t())) #[BL, num_embeddings]\n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1) #[BL, 1]\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings).to(device)# [BL, num_embeddings]\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "        #print(encodings.shape) [250, 512]\n",
    "        # Use EMA to update the embedding vectors\n",
    "        if self.training:\n",
    "            self._ema_cluster_size = self._ema_cluster_size * self._decay + \\\n",
    "                                     (1 - self._decay) * torch.sum(encodings, 0)\n",
    "            #print(self._ema_cluster_size.shape) [512]\n",
    "            n = torch.sum(self._ema_cluster_size)\n",
    "            self._ema_cluster_size = (\n",
    "                (self._ema_cluster_size + self._epsilon)\n",
    "                / (n + self._num_embeddings * self._epsilon) * n)\n",
    "            \n",
    "            dw = torch.matmul(encodings.t(), flat_input)\n",
    "            self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw)\n",
    "            \n",
    "            self._embedding.weight = nn.Parameter(self._ema_w / self._ema_cluster_size.unsqueeze(1))\n",
    "        \n",
    "        # Quantize and unflatten\n",
    "        #encodings.shape = [BL, num_embeddings] , weight.shape=[num_embeddings, C]\n",
    "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
    "\n",
    "        \n",
    "        # Loss\n",
    "        e_latent_loss = torch.mean((quantized.detach() - inputs)**2)\n",
    "        q_latent_loss = torch.mean((quantized - inputs.detach())**2)\n",
    "#        print(q_latent_loss.item(), 0.25 * e_latent_loss.item())\n",
    "        loss = q_latent_loss + self._commitment_cost * e_latent_loss\n",
    "        \n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "        # convert quantized from BLC -> BCL\n",
    "        return loss, quantized.permute(0, 2, 1).contiguous(), perplexity\n",
    "    '''\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # convert inputs from BCL -> BLC\n",
    "        inputs = inputs.permute(0, 2, 1).contiguous()\n",
    "        input_shape = inputs.shape\n",
    "        # Flatten input\n",
    "        flat_input = inputs.view(-1, self._embedding_dim)     #[BL, C]\n",
    "        # Calculate distances\n",
    "        \n",
    "        distances = torch.norm(flat_input.unsqueeze(1) - self._embedding.weight, dim=2, p=2)\n",
    " #       distances = (torch.sum(flat_input**2, dim=1, keepdim=True) \n",
    " #                   + torch.sum(self._embedding.weight**2, dim=1)\n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1) #[BL, 1]\n",
    "        \n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings).to(device)# [BL, num_embeddings]\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "        #print(encodings.shape) [250, 512]\n",
    "\n",
    "#         # Use EMA to update the embedding vectors\n",
    "#         if self.training:\n",
    "#             self._ema_cluster_size = self._ema_cluster_size * self._decay + \\\n",
    "#                                      (1 - self._decay) * torch.sum(encodings, 0)\n",
    "#             #print(self._ema_cluster_size.shape) [512]\n",
    "#             n = torch.sum(self._ema_cluster_size)\n",
    "#             self._ema_cluster_size = (\n",
    "#                 (self._ema_cluster_size + self._epsilon)\n",
    "#                 / (n + self._num_embeddings * self._epsilon) * n)\n",
    "            \n",
    "#             dw = torch.matmul(encodings.t(), flat_input)\n",
    "#             self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw)\n",
    "            \n",
    "#             self._embedding.weight = nn.Parameter(self._ema_w / self._ema_cluster_size.unsqueeze(1))\n",
    "\n",
    "        # Quantize and unflatten\n",
    "        #encodings.shape = [BL, num_embeddings] , weight.shape=[num_embeddings, C]\n",
    "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
    "        # Loss\n",
    "        e_latent_loss = torch.mean((quantized.detach() - inputs)**2)\n",
    "        q_latent_loss = torch.mean((quantized - inputs.detach())**2)\n",
    "#        print(q_latent_loss.item(), 0.25 * e_latent_loss.item())\n",
    "        loss = q_latent_loss + self._commitment_cost * e_latent_loss\n",
    "        \n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "        # same as torch.exp( entropy loss )\n",
    "        \n",
    "        # convert quantized from BLC -> BCL\n",
    "        return loss, quantized.permute(0, 2, 1).contiguous(), perplexity\n",
    "#    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder & Decoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Audio encoder\n",
    "    The vq-vae paper says that the encoder has 6 strided convolutions with stride 2 and window-size 4.\n",
    "    The number of channels and a nonlinearity is not specified in the paper. \n",
    "    I tried using ReLU, it didn't work.\n",
    "    Now I try using tanh, hoping that this will keep my encoded values within the neighborhood of 0,\n",
    "    so they do not drift too far away from encoding vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoding_channels, in_channels=256):\n",
    "        super(Encoder,self).__init__()\n",
    "        self._num_layers = 2 * len(encoding_channels)\n",
    "        self._layers = nn.ModuleList()\n",
    "        for out_channels in encoding_channels:\n",
    "            self._layers.append(nn.Conv1d(in_channels=in_channels,\n",
    "                                    out_channels=out_channels,\n",
    "                                    stride=2,\n",
    "                                    kernel_size=4,\n",
    "                                    padding=0, \n",
    "                                        ))\n",
    "            self._layers.append(nn.Tanh())\n",
    "            in_channels = out_channels\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(self._num_layers):\n",
    "            x = self._layers[i](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoding_channels,\n",
    "                 num_embeddings, \n",
    "                 embedding_dim,\n",
    "                 commitment_cost, \n",
    "                 layers,\n",
    "                 stacks,\n",
    "                 kernel_size,\n",
    "                 decay=0):\n",
    "        super(Model, self).__init__()       \n",
    "        self._encoder = Encoder(encoding_channels=encoding_channels)\n",
    "        #I tried adding batch normalization here, because:\n",
    "        #the distribution of encoded values needs to be similar to the distribution of embedding vectors\n",
    "        #otherwise we'll see \"posterior collapse\": all values will be assigned to the same embedding vector,\n",
    "        #and stay that way (because vectors which do not get assigned anything do not get updated).\n",
    "        #Batch normalization is a way to fix that. But it didn't work: model\n",
    "        #reproduced voice correctly, but the words were completely wrong.\n",
    "        #self._batch_norm = nn.BatchNorm1d(1)\n",
    "        if decay > 0.0:\n",
    "#             self._vq_vae = EMVectorQuantizerEMA(num_embeddings, embedding_dim, \n",
    "#                                               commitment_cost, decay, 100)\n",
    "            self._vq_vae = VectorQuantizerEMA(num_embeddings, embedding_dim, \n",
    "                                               commitment_cost, decay)\n",
    "\n",
    "        else:\n",
    "            self._vq_vae = VectorQuantizer(num_embeddings, embedding_dim,\n",
    "                                           commitment_cost)\n",
    "        self._decoder = WaveNet(device, out_channels=256, #dimension of ohe mu-quantized signal\n",
    "                                layers=layers, #like in original WaveNet\n",
    "                                stacks=stacks,\n",
    "                                residual_channels=512,\n",
    "                                gate_channels=512,\n",
    "                                skip_out_channels=512,\n",
    "                                kernel_size=kernel_size, \n",
    "                                dropout=1 - 0.95,\n",
    "                                cin_channels=embedding_dim, #local conditioning channels - on encoder output\n",
    "                                gin_channels=number_of_speakers, #global conditioning channels - on speaker_id\n",
    "                                n_speakers=number_of_speakers,\n",
    "                                weight_normalization=False, \n",
    "                                upsample_conditional_features=True, \n",
    "                                decoding_channels=encoding_channels[::-1],\n",
    "                                use_speaker_embedding=False\n",
    "                               )\n",
    "        self.recon_loss = torch.nn.CrossEntropyLoss()\n",
    "        self.receptive_field = receptive_field_size(total_layers=layers, num_cycles=stacks, kernel_size=kernel_size)\n",
    "#        self.mean = None\n",
    "#        self.std = None\n",
    "    def forward(self, x):\n",
    "        audio, target, speaker_id = x\n",
    "        assert len(audio.shape) == 3 # B x C x L \n",
    "        assert audio.shape[1] == 256\n",
    "        z = self._encoder(audio)\n",
    "        #normalize output - subtract mean, divide by standard deviation\n",
    "        #without this, perplexity goes to 1 almost instantly\n",
    "#         if self.mean is None:\n",
    "#             self.mean = z.mean().detach()\n",
    "#         if self.std is None:\n",
    "#              self.std = z.std().detach()\n",
    "#        z = z - self.mean\n",
    "#        z = z / self.std\n",
    "        vq_loss, quantized, perplexity = self._vq_vae(z)\n",
    "#        assert z.shape == quantized.shape\n",
    "#        print(\"audio.shape\", audio.shape)\n",
    "#        print(\"quantized.shape\", quantized.shape)\n",
    "        x_recon = self._decoder(audio, quantized, speaker_id, softmax=False)\n",
    "        x_recon = x_recon[:, :, self.receptive_field:-1]\n",
    "        recon_loss_value = self.recon_loss(x_recon, target[:, 1:])\n",
    "        loss = recon_loss_value + vq_loss\n",
    "        \n",
    "        return loss, recon_loss_value, x_recon, perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_updates = 39818\n",
    "#vector quantizer parameters:\n",
    "embedding_dim = 64 #dimension of each vector\n",
    "encoding_channels = [512,512,512,512,512,embedding_dim]\n",
    "num_embeddings = 512 #number of vectors\n",
    "commitment_cost = 0.25\n",
    "\n",
    "#wavenet parameters:\n",
    "kernel_size=2\n",
    "total_layers=30\n",
    "num_cycles=3\n",
    "\n",
    "\n",
    "decay = 0.99\n",
    "#decay = 0\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3070\n"
     ]
    }
   ],
   "source": [
    "receptive_field = receptive_field_size(total_layers=total_layers, num_cycles=num_cycles, kernel_size=kernel_size)\n",
    "print(receptive_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(num_embeddings=num_embeddings,\n",
    "              encoding_channels=encoding_channels,\n",
    "              embedding_dim=embedding_dim, \n",
    "              commitment_cost=commitment_cost, \n",
    "              layers=total_layers,\n",
    "              stacks=num_cycles,\n",
    "              kernel_size=kernel_size,\n",
    "              decay=decay).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1, amsgrad=False)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer,\n",
    "                                              lr_lambda=lambda epoch: 1e-3 if epoch == 0 else  (optimizer.param_groups[0]['lr'] - (1e-3 - 1e-6)/500) if epoch <= 500 else optimizer.param_groups[0]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastdtw import fastdtw\n",
    "\n",
    "def conversion(original_wav, speaker):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated_file = np.array([])\n",
    "        wav, sr = librosa.load(original_wav)\n",
    "        speaker_index = speaker_dic[speaker]\n",
    "        \n",
    "        normalized = librosa.util.normalize(wav)\n",
    "        quantized = quantize_data(normalized, 256)\n",
    "\n",
    "        for i in range(0, len(wav), 16126):\n",
    "            sample = quantized[i:i+16126]\n",
    "            if (len(sample)!= 16126):\n",
    "                sample = np.append(sample, np.zeros(16126 - len(sample)).astype(int))\n",
    "                \n",
    "            sample = torch.from_numpy(sample)\n",
    "            ohe_audio = torch.FloatTensor(256, 16126).zero_()\n",
    "            ohe_audio.scatter_(0, sample.unsqueeze(0), 1.)\n",
    "            \n",
    "            speaker_id = torch.from_numpy(np.array(speaker_index)).unsqueeze(0).unsqueeze(0)\n",
    "            ohe_speaker = torch.FloatTensor(number_of_speakers, 1).zero_()\n",
    "            ohe_speaker.scatter_(0, speaker_id, 1.)\n",
    "\n",
    "            ohe_audio = ohe_audio.unsqueeze(0).to(device)\n",
    "            ohe_speaker = ohe_speaker.unsqueeze(0).to(device)\n",
    "            encoded = model._encoder(ohe_audio)\n",
    "            _, valid_quantize, _ = model._vq_vae(encoded)\n",
    "            \n",
    "            valid_reconstructions = model._decoder.incremental_forward(ohe_audio[:,:,0:1], \n",
    "                                                               valid_quantize, \n",
    "                                                               ohe_speaker, \n",
    "                                                               T=16126)\n",
    "            \n",
    "            \n",
    "            recon = valid_reconstructions.squeeze().argmax(dim=0).detach().cpu().numpy()\n",
    "            mu_encoded = (recon + 1) / 128 - 1\n",
    "            mu_decoded = mu_law_decode(recon, mu=256)\n",
    "            generated_file = np.append(generated_file, mu_encoded)\n",
    "        # librosa.output.write_wav(\"generated.wav\",  generated_file, sr=sr)\n",
    "        return wav, generated_file, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mu_law_encode(data, mu):\n",
    "    mu_x = np.sign(data) * np.log(1 + mu * np.abs(data)) / np.log(mu + 1)\n",
    "    return mu_x\n",
    "\n",
    "def mu_law_decode(mu_x, mu):\n",
    "    data = np.sign(mu_x) * (1 / mu) * ((1 + mu) ** np.abs(mu_x) - 1)\n",
    "    return data\n",
    "\n",
    "def quantize_data(data, classes):\n",
    "    mu_x = mu_law_encode(data, classes)\n",
    "    bins = np.linspace(-1, 1, classes)\n",
    "    quantized = np.digitize(mu_x, bins) - 1\n",
    "    return quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load(\"model_epoch440\", map_location=\"cuda:0\"))\n",
    "model.load_state_dict(torch.load(\"model_epoch\"+str(args.load), map_location=\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Plot Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(original_wav, speaker, filename = \"generated.wav\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated_file = np.array([])\n",
    "        wav, sr = librosa.load(original_wav)\n",
    "        print(len(wav))\n",
    "        speaker_index = speaker_dic[speaker]\n",
    "        print(speaker_index)\n",
    "        normalized = librosa.util.normalize(wav)\n",
    "        quantized = quantize_data(normalized, 256)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(0, len(wav), 16126):\n",
    "            sample = quantized[i:i+16126]\n",
    "            \n",
    "            \n",
    "            if (len(sample)!= 16126):\n",
    "                sample = np.append(sample, np.zeros(16126 - len(sample)).astype(int))\n",
    "                print(16126 - len(sample))\n",
    "                \n",
    "            sample = torch.from_numpy(sample)\n",
    "            print(sample)\n",
    "            print(torch.max(sample), torch.min(sample))\n",
    "            ohe_audio = torch.FloatTensor(256, 16126).zero_()\n",
    "            ohe_audio.scatter_(0, sample.unsqueeze(0), 1.)\n",
    "            \n",
    "            speaker_id = torch.from_numpy(np.array(speaker_index)).unsqueeze(0).unsqueeze(0).long()\n",
    "            ohe_speaker = torch.FloatTensor(number_of_speakers, 1).zero_()\n",
    "            ohe_speaker.scatter_(0, speaker_id, 1.)\n",
    "\n",
    "            ohe_audio = ohe_audio.unsqueeze(0).to(device)\n",
    "            ohe_speaker = ohe_speaker.unsqueeze(0).to(device)\n",
    "            encoded = model._encoder(ohe_audio)\n",
    "            _, valid_quantize, _ = model._vq_vae(encoded)\n",
    "            \n",
    "            valid_reconstructions = model._decoder.incremental_forward(ohe_audio[:,:,0:1], \n",
    "                                                               valid_quantize, \n",
    "                                                               ohe_speaker, \n",
    "                                                               T=16126)\n",
    "            \n",
    "            \n",
    "#         for i in range(0, len(wav), 16126):\n",
    "#             sample = quantized[i:i+16126]\n",
    "#             if (len(sample)!= 16126):\n",
    "#                 sample = np.append(sample, np.zeros(16126 - len(sample)).astype(int))\n",
    "                \n",
    "#             sample = torch.from_numpy(sample)\n",
    "#             print(sample)\n",
    "#             print(torch.max(sample), torch.min(sample))\n",
    "#             ohe_audio = torch.FloatTensor(256, 16126).zero_()\n",
    "#             ohe_audio.scatter_(0, sample.unsqueeze(0), 1.)\n",
    "            \n",
    "#             speaker_id = torch.from_numpy(np.array(speaker_index)).unsqueeze(0).unsqueeze(0)\n",
    "#             ohe_speaker = torch.FloatTensor(number_of_speakers, 1).zero_()\n",
    "#             ohe_speaker.scatter_(0, speaker_id, 1.)\n",
    "\n",
    "#             ohe_audio = ohe_audio.unsqueeze(0).to(device)\n",
    "#             ohe_speaker = ohe_speaker.unsqueeze(0).to(device)\n",
    "#             encoded = model._encoder(ohe_audio)\n",
    "#             _, valid_quantize, _ = model._vq_vae(encoded)\n",
    "            \n",
    "#             valid_reconstructions = model._decoder.incremental_forward(ohe_audio[:,:,0:1], \n",
    "#                                                                valid_quantize, \n",
    "#                                                                ohe_speaker, \n",
    "#                                                                T=16126)\n",
    "            \n",
    "                \n",
    "            recon = valid_reconstructions.squeeze().argmax(dim=0).detach().cpu().numpy()\n",
    "            mu_encoded = (recon + 1) / 128 - 1\n",
    "            mu_decoded = mu_law_decode(mu_encoded, mu=256)\n",
    "            generated_file = np.append(generated_file, mu_decoded)\n",
    "        print(len(generated_file))\n",
    "        librosa.output.write_wav(filename,  generated_file, sr=sr)\n",
    "    return generated_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = model._vq_vae._embedding.weight.data.cpu()\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "y = pca.fit_transform(embeddings)\n",
    "plt.scatter(y[:,0],y[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.0209e-41,  7.0209e-41,  7.0209e-41,  ...,  7.0209e-41,\n",
       "          7.0209e-41, -7.0209e-41],\n",
       "        [ 7.0209e-41, -7.0209e-41, -7.0209e-41,  ..., -7.0209e-41,\n",
       "         -7.0209e-41, -7.0209e-41],\n",
       "        [-7.0209e-41, -7.0209e-41,  7.0209e-41,  ..., -7.0209e-41,\n",
       "          7.0209e-41,  7.0209e-41],\n",
       "        ...,\n",
       "        [ 7.8871e-01,  1.9195e-01, -4.5106e-01,  ...,  6.7328e-01,\n",
       "         -5.9341e-01, -7.2493e-01],\n",
       "        [ 9.0710e-01,  6.5645e-01, -8.6369e-01,  ...,  6.8986e-01,\n",
       "         -8.7497e-01, -9.0332e-01],\n",
       "        [-8.9677e-01,  1.0546e-01, -3.6687e-01,  ...,  2.8703e-01,\n",
       "          5.8321e-01,  2.7874e-01]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45237\n",
      "1\n",
      "tensor([146, 154, 152,  ...,  96, 175, 129])\n",
      "tensor(255) tensor(2)\n",
      "tensor([ 73, 190, 147,  ..., 106, 142, 151])\n",
      "tensor(250) tensor(8)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-700fc5e743eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreconstructions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_wav\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./VCTK/wav48/p225/p225_001.wav'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspeaker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'p226'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"generated1.wav\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model_epoch\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"cuda:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mreconstructions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_wav\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./VCTK/wav48/p225/p225_001.wav'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspeaker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'p226'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"generated2.wav\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model_epoch\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"cuda:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mreconstructions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_wav\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./VCTK/wav48/p225/p225_001.wav'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspeaker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'p226'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"generated3.wav\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-59b4ad97b440>\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(original_wav, speaker, filename)\u001b[0m\n\u001b[0;32m     38\u001b[0m                                                                \u001b[0mvalid_quantize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m                                                                \u001b[0mohe_speaker\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m                                                                T=16126)\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\pytorch_vq_vae_audio\\pytorch_vq_vae_audio\\wavenet_vocoder\\wavenet.py\u001b[0m in \u001b[0;36mincremental_forward\u001b[1;34m(self, initial_input, c, g, T, test_inputs, tqdm, softmax, quantize, log_scale_min)\u001b[0m\n\u001b[0;32m    310\u001b[0m             \u001b[0mskips\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                 \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mincremental_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m                 \u001b[0mskips\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mskips\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mskips\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\pytorch_vq_vae_audio\\pytorch_vq_vae_audio\\wavenet_vocoder\\modules.py\u001b[0m in \u001b[0;36mincremental_forward\u001b[1;34m(self, x, c, g)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mincremental_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_incremental\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\pytorch_vq_vae_audio\\pytorch_vq_vae_audio\\wavenet_vocoder\\modules.py\u001b[0m in \u001b[0;36m_forward\u001b[1;34m(self, x, c, g, is_incremental)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1x1c\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m             \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_conv1x1_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1x1c\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_incremental\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m             \u001b[0mca\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplitdim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msplitdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mca\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\pytorch_vq_vae_audio\\pytorch_vq_vae_audio\\wavenet_vocoder\\modules.py\u001b[0m in \u001b[0;36m_conv1x1_forward\u001b[1;34m(conv, x, is_incremental)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \"\"\"\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_incremental\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mincremental_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\pytorch_vq_vae_audio\\pytorch_vq_vae_audio\\wavenet_vocoder\\conv.py\u001b[0m in \u001b[0;36mincremental_forward\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m#print(\"input_data.shape\", input_data.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclear_buffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reconstructions = generate(original_wav = './VCTK/wav48/p225/p225_001.wav', speaker = 'p226', filename = \"generated1.wav\")\n",
    "model.load_state_dict(torch.load(\"model_epoch\"+str(args.load), map_location=\"cuda:0\"))\n",
    "reconstructions = generate(original_wav = './VCTK/wav48/p225/p225_001.wav', speaker = 'p226', filename = \"generated2.wav\")\n",
    "model.load_state_dict(torch.load(\"model_epoch\"+str(args.load), map_location=\"cuda:0\"))\n",
    "reconstructions = generate(original_wav = './VCTK/wav48/p225/p225_001.wav', speaker = 'p226', filename = \"generated3.wav\")\n",
    "model.load_state_dict(torch.load(\"model_epoch\"+str(args.load), map_location=\"cuda:0\"))\n",
    "reconstructions = generate(original_wav = './VCTK/wav48/p225/p225_001.wav', speaker = 'p226', filename = \"generated4.wav\")\n",
    "model.load_state_dict(torch.load(\"model_epoch\"+str(args.load), map_location=\"cuda:0\"))\n",
    "reconstructions = generate(original_wav = './VCTK/wav48/p225/p225_001.wav', speaker = 'p226', filename = \"generated5.wav\")\n",
    "model.load_state_dict(torch.load(\"model_epoch\"+str(args.load), map_location=\"cuda:0\"))\n",
    "reconstructions = generate(original_wav = './VCTK/wav48/p225/p225_001.wav', speaker = 'p226', filename = \"generated6.wav\")\n",
    "model.load_state_dict(torch.load(\"model_epoch\"+str(args.load), map_location=\"cuda:0\"))\n",
    "reconstructions = generate(original_wav = './VCTK/wav48/p225/p225_001.wav', speaker = 'p226', filename = \"generated7.wav\")\n",
    "model.load_state_dict(torch.load(\"model_epoch\"+str(args.load), map_location=\"cuda:0\"))\n",
    "reconstructions = generate(original_wav = './VCTK/wav48/p225/p225_001.wav', speaker = 'p226', filename = \"generated8.wav\")\n",
    "model.load_state_dict(torch.load(\"model_epoch\"+str(args.load), map_location=\"cuda:0\"))\n",
    "reconstructions = generate(original_wav = './VCTK/wav48/p225/p225_001.wav', speaker = 'p226', filename = \"generated9.wav\")\n",
    "model.load_state_dict(torch.load(\"model_epoch\"+str(args.load), map_location=\"cuda:0\"))\n",
    "reconstructions = generate(original_wav = './VCTK/wav48/p225/p225_001.wav', speaker = 'p226', filename = \"generated10.wav\")\n",
    "model.load_state_dict(torch.load(\"model_epoch\"+str(args.load), map_location=\"cuda:0\"))\n",
    "reconstructions = generate(original_wav = './VCTK/wav48/p225/p225_001.wav', speaker = 'p226', filename = \"generated11.wav\")\n",
    "model.load_state_dict(torch.load(\"model_epoch\"+str(args.load), map_location=\"cuda:0\"))\n",
    "reconstructions = generate(original_wav = './VCTK/wav48/p225/p225_001.wav', speaker = 'p226', filename = \"generated12.wav\")\n",
    "model.load_state_dict(torch.load(\"model_epoch\"+str(args.load), map_location=\"cuda:0\"))\n",
    "reconstructions = generate(original_wav = './VCTK/wav48/p225/p225_001.wav', speaker = 'p226', filename = \"generated13.wav\")\n",
    "model.load_state_dict(torch.load(\"model_epoch\"+str(args.load), map_location=\"cuda:0\"))\n",
    "reconstructions = generate(original_wav = './VCTK/wav48/p225/p225_001.wav', speaker = 'p226', filename = \"generated14.wav\")\n",
    "model.load_state_dict(torch.load(\"model_epoch\"+str(args.load), map_location=\"cuda:0\"))\n",
    "reconstructions = generate(original_wav = './VCTK/wav48/p225/p225_001.wav', speaker = 'p226', filename = \"generated15.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "original_wav = './VCTK/wav48/p225/p225_002.wav'\n",
    "speaker_index = speaker_dic['p226']\n",
    "\n",
    "wav, sr = librosa.load(original_wav)\n",
    "print(len(wav))\n",
    "speaker = np.eye(number_of_speakers)[speaker_index]\n",
    "speaker = torch.tensor(speaker)\n",
    "speaker = speaker.unsqueeze(0)\n",
    "\n",
    "normalized = librosa.util.normalize(wav)\n",
    "quantized = quantize_data(normalized, 256)\n",
    "\n",
    "max_audio_start = quantized.shape[0] - 16126\n",
    "if (max_audio_start > 0):\n",
    "    audio_start = random.randint(0, max_audio_start)\n",
    "    sample = quantized[audio_start:audio_start+16126]\n",
    "else :\n",
    "    sample = np.append(quantized, np.zeros(16126 - len(quantized))).astype(int)\n",
    "\n",
    "sample = torch.from_numpy(sample)\n",
    "ohe_audio = torch.FloatTensor(256, 16126).zero_()\n",
    "ohe_audio.scatter_(0, sample.unsqueeze(0), 1.)\n",
    "\n",
    "speaker_id = torch.from_numpy(np.array(speaker_index)).unsqueeze(0).unsqueeze(0).long()\n",
    "ohe_speaker = torch.FloatTensor(number_of_speakers, 1).zero_()\n",
    "ohe_speaker.scatter_(0, speaker_id, 1.)\n",
    "\n",
    "\n",
    "valid_originals = ohe_audio.to(device).unsqueeze(0)\n",
    "speaker_id = ohe_speaker.to(device).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoded = model._encoder(valid_originals)\n",
    "\n",
    "    _, valid_quantize, _ = model._vq_vae(encoded)\n",
    "    #valid_reconstructions = model._decoder(valid_originals, valid_quantize, speaker_id) - this one works fine\n",
    "    valid_reconstructions = model._decoder.incremental_forward(valid_originals[:,:,0:1], \n",
    "                                                               valid_quantize, \n",
    "                                                               speaker_id, \n",
    "                                                               T=16126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(valid_quantize[:,0,:].detach().cpu().numpy().ravel())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "recon = valid_reconstructions.squeeze().argmax(dim=0).detach().cpu().numpy()\n",
    "plt.plot(recon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orig = valid_originals.squeeze().argmax(dim=0).detach().cpu().numpy()\n",
    "#plt.plot(valid_quantize.detach().numpy().ravel())\n",
    "plt.plot(orig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon = valid_reconstructions.squeeze().argmax(dim=0).detach().cpu().numpy()\n",
    "mu_encoded = (recon + 1) / 128 - 1\n",
    "mu_decoded = mu_law_decode(mu_encoded, mu=256)\n",
    "plt.plot(mu_decoded[2000:4000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recon = valid_originals.squeeze().argmax(dim=0).detach().cpu().numpy()\n",
    "mu_encoded_orig = (recon + 1) / 128 - 1\n",
    "mu_decoded_orig = mu_law_decode(mu_encoded_orig, mu=256)\n",
    "plt.plot(mu_decoded_orig[2000:4000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(mu_decoded, rate=22050))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(mu_decoded_orig, rate=22050))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"model_epoch\"+str(495), map_location=\"cuda:0\"))\n",
    "# model.load_state_dict(torch.load(\"mymodel\", map_location=\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model._vq_vae._embedding.weight.data.cpu()\n",
    "#print(embeddings)\n",
    "#print(embeddings.shape)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "y = pca.fit_transform(embeddings)\n",
    "plt.scatter(y[:,0],y[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
